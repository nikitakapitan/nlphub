{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this colab we:\n",
    "1. Import CNN dailymail (news, summarization) pairs.\n",
    "2. SUMMARIZE first example (about Hary Potter)\n",
    "    - baseline = first 3 senteces of article\n",
    "    - GPT-2 = append TL;DR and generate next tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets rouge_score py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Found cached dataset cnn_dailymail (C:/Users/nikit/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5856b8e7cee48aa8363638c471f0111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['article', 'highlights', 'id']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import transformers\n",
    "from datasets import load_metric\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel ...\n",
      "- - - - -\n",
      "Harry Potter star Daniel Radcliffe gets Â£20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "print(sample['article'][29:211], '...')\n",
    "print('- - - - -')\n",
    "print(sample['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "\n",
    "# Baseline = first 3 senteces\n",
    "def three_sentence_summary(text):\n",
    "    return \" \".join(nltk.sent_tokenize(text)[:3])\n",
    "summaries['baseline'] = three_sentence_summary(sample['article'][:1000])\n",
    "\n",
    "# GPT-2 (not trained to SUMMARIZE but at least we can give a shot by appending \"TL;DR\")\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    gpt2_query = sample['article']  + \"\\nTL;DR:\\n\"\n",
    "    gpt2_pipe = transformers.pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "    gpt2_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "    summaries['gpt2'] = \"\".join(nltk.sent_tokenize(gpt2_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
    "\n",
    "# T5. \"summarization\" pipeline is ~ T5Model.from_pretrained('t5-large') + prompt: \"summarize: <ARTICLE>\"\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    t5_pipe = transformers.pipeline(\"summarization\", model=\"t5-large\")\n",
    "    t5_out = pipe(sample['article'])\n",
    "    summaries['t5'] = \"\".join(nltk.sent_tokenize(t5_out[0][\"summary_text\"]))\n",
    "\n",
    "# BART fine-tuned on the CNN/DailyMail\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    bart_pipe = transformers.pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    bart_out = pipe(sample['article'])\n",
    "    summaries['bart'] = \"\".join(nltk.sent_tokenize(bart_out[0][\"summary_text\"]))\n",
    "\n",
    "# PEGASUS\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    pegasus_pipe = transformers.pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "    pegasus_out = pipe(sample['article'])\n",
    "    summaries['pegasus'] = \"\".join(nltk.sent_tokenize(pegasus_out[0][\"summary_text\"])).replace(\" .<n>\", \".\\n\")\n",
    "\n",
    "print(\"GROUND TRUTH\")\n",
    "print(sample['highlights'], '\\n')\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- GPT-2 instead of text summary, summarized the characters, aka GPT-2 \"hallucination\"\n",
    "\n",
    "\n",
    "# Let's compute BLEU and also ROUGE\n",
    "\n",
    "First on Harry Potter sample, then on the whole CNN/DailyMail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = load_metric(\"sacrebleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "bleus= []\n",
    "rouge_records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    # bleu.add(prediction=summaries[model_name], reference=sample['highlights'])\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=sample['highlights'])\n",
    "\n",
    "    # bleus.append(bleu.compute())\n",
    "    rouge_score = rouge_metric.compute()\n",
    "\n",
    "    rouge_dict = dict((rn, rouge_score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    rouge_records.append(rouge_dict)\n",
    "\n",
    "# pd.DataFrame.from_records(records, index=summaries.keys())\n",
    "# pd.DataFrame(bleus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.390732</td>\n",
       "      <td>0.176082</td>\n",
       "      <td>0.248922</td>\n",
       "      <td>0.324405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.390732  0.176082  0.248922   0.324405"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric,\n",
    "                                column_text=\"article\",\n",
    "                                column_summary=\"highlights\"):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions=summaries,\n",
    "                     references=dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "test_sampled = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"])\n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "pegasus_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "pegasus_model = AutoModelForSeq2SeqLM.from_pertrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, pegasus_model, pegasus_tokenizer, batch_size=8)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "# published paper results: \n",
    "# R1 - 0.439, R2 - 0.212, RL - 0.407\n",
    "pd.DataFrame(rouge_dict, index=['pegasus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Pegasus\n",
    "Consider Summarization for another dataset : Dialogues (SAMSun).\n",
    "- The summarization should be more abstract and written from third-person-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (C:/Users/nikit/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39a82d6717d40c887734addd0827655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum = load_dataset('samsum')\n",
    "print(dataset_samsum)\n",
    "\n",
    "samsum_sample = dataset_samsum['test'][0]\n",
    "print('Dialogue:')\n",
    "print(samsum_sample['dialogue'])\n",
    "print('\\nSummary:')\n",
    "print(samsum_sample['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zero-shot PEGASUS on Hannah example\n",
    "\n",
    "pegasus_out = pegasus_pipe(dataset_samsum['test'][0]['dialogue'])\n",
    "print('Pegasus summary:')\n",
    "print(pegasus_out[0]['summary_text'].replace(' .<n>', '.\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the model tries to summarize by extracting the key sentences. \n",
    "- That is OK for CNN/DailyMail but not SAMSum\n",
    "\n",
    "Let's compute zero-shot **Rouge** of PEGASUS on whole SAMSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                                   tokenizer, column_text=\"dialogue\",\n",
    "                                   column_summary=\"summary\", batch_size=8)\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n",
    "\n",
    "# HG book: R1 - 0.296, R2 - 0.088, RL - 0.230, RLsum - 0.230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_dataset_to_tensors(batch, tokenizer):\n",
    "\n",
    "    input_encodings = tokenizer(batch['dialogue'], max_length=1024, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(batch['summary'], max_length=128, truncation=True)\n",
    "    \n",
    "    return {'inputs_ids' : input_encodings['input_ids'],\n",
    "            'attention_mask' : input_encodings['attention_mask'],\n",
    "            'labels' : target_encodings['input_ids']}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(cast_dataset_to_tensors, batched=True)\n",
    "\n",
    "columns = ['input_ids', 'labels', 'attention_mask']\n",
    "dataset_samsun_pt.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataccolator:\n",
    "- stack all tensors from batch\n",
    "- prepare decoder targets (teacher forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(pegasus_tokenizer, model=pegasus_model)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainings_args = TrainingArguments(output_dir='pegasus-samsum', num_train_epochs=1,\n",
    "warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "weight_decay=0.01, logging_steps=10, push_to_hub=True, evaluation_strategy='steps',\n",
    "eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
